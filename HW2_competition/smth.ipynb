{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Copy of 06.time-series-anomaly-detection-ecg.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "3RY_N3gOmfDi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "edf1b1bd-b376-4c9f-cac5-0ee8e27ce21e"
   },
   "source": [
    "import torch\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import nn\n",
    "import pickle as pk\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "RANDOM_SEED = 2077\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ],
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x20da5ca3250>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DFWsBcdWjDkU"
   },
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "             y_true        length\ncount  11324.000000  11324.000000\nmean       0.029053    777.405864\nstd        0.167963    336.557775\nmin        0.000000     94.000000\n25%        0.000000    539.000000\n50%        0.000000    739.000000\n75%        0.000000    746.000000\nmax        1.000000   2044.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y_true</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>11324.000000</td>\n      <td>11324.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.029053</td>\n      <td>777.405864</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.167963</td>\n      <td>336.557775</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>94.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>539.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>739.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>746.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>2044.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('data/train.csv')\n",
    "df1['length']=df1['request'].apply(len)\n",
    "df1.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('data/test.csv')\n",
    "df_test['length']=df_test['request'].apply(len)\n",
    "# df_short_test=df_test[df_test['length']<df_test['length'].quantile(0.89)]\n",
    "# df_short_test.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "             y_true        length\ncount  10075.000000  10075.000000\nmean       0.032655    670.735583\nstd        0.177741    117.536675\nmin        0.000000     94.000000\n25%        0.000000    533.000000\n50%        0.000000    726.000000\n75%        0.000000    742.000000\nmax        1.000000   1299.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y_true</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>10075.000000</td>\n      <td>10075.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.032655</td>\n      <td>670.735583</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.177741</td>\n      <td>117.536675</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>94.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>533.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>726.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.000000</td>\n      <td>742.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1299.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_short=df1[df1['length']<df1['length'].quantile(0.89)]\n",
    "df_short.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "(9746, 1)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_data=df_short[df_short['y_true']==0.0].drop(['y_true','length'],axis=1)\n",
    "norm_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "(329, 1)"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_data=df_short[df_short['y_true']==1.0].drop(['y_true','length'],axis=1)\n",
    "anomaly_data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "X_train, X_check = train_test_split(norm_data,test_size=0.15,random_state=RANDOM_SEED)\n",
    "X_check, X_test = train_test_split(X_check,test_size=0.33, random_state=RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                request\n6753  \\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...\n3633  \\nThu, 15 Mar 2018 14:45:52 INFO\\nGET /vulnban...\n2403  \\nThu, 15 Mar 2018 14:45:52 INFO\\nGET /vulnban...\n220   \\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...\n6593  \\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...\n...                                                 ...\n654   \\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...\n5162  \\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...\n3696  \\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...\n2144  \\nThu, 15 Mar 2018 14:45:52 INFO\\nGET /vulnban...\n167   \\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...\n\n[8284 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>request</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6753</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...</td>\n    </tr>\n    <tr>\n      <th>3633</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nGET /vulnban...</td>\n    </tr>\n    <tr>\n      <th>2403</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nGET /vulnban...</td>\n    </tr>\n    <tr>\n      <th>220</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...</td>\n    </tr>\n    <tr>\n      <th>6593</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>654</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...</td>\n    </tr>\n    <tr>\n      <th>5162</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...</td>\n    </tr>\n    <tr>\n      <th>3696</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...</td>\n    </tr>\n    <tr>\n      <th>2144</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nGET /vulnban...</td>\n    </tr>\n    <tr>\n      <th>167</th>\n      <td>\\nThu, 15 Mar 2018 14:45:52 INFO\\nPOST /vulnba...</td>\n    </tr>\n  </tbody>\n</table>\n<p>8284 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "((8284, 1200),\n (483, 1200),\n (329, 1200),\n (979, 1200),\n pandas.core.frame.DataFrame)"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_fit = tfidf_vectorizer.fit(X_train['request'])\n",
    "X_train_vec = pd.DataFrame(tfidf_vectorizer.transform(X_train['request']).todense())\n",
    "X_test_norm_vec = pd.DataFrame(tfidf_vectorizer.transform(X_test['request']).todense())\n",
    "X_check_vec = pd.DataFrame(tfidf_vectorizer.transform(X_check['request']).todense())\n",
    "X_test_anom_vec = pd.DataFrame(tfidf_vectorizer.transform(anomaly_data['request']).todense())\n",
    "X_train_vec.shape,X_test_norm_vec.shape,X_test_anom_vec.shape,X_check_vec.shape,type(X_check_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "TfidfVectorizer()"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_fit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "pk.dump(tfidf_fit, open(\"tfidf.pkl\",\"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "(11764, 1200)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vocab = pk.load(open(\"tfidf.pkl\", 'rb'))\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary=tfidf_vocab.vocabulary_)\n",
    "df_ = pd.DataFrame(tfidf_vectorizer.fit_transform(df_test['request']).todense())\n",
    "df_.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "(11764, 1200)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_vec=pd.DataFrame(tfidf_vectorizer.transform(df_test['request']).todense())\n",
    "df_test_vec.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "((8284, 585), (483, 585), (329, 585), (979, 585), pandas.core.frame.DataFrame)"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca=PCA(n_components=0.99)\n",
    "pca_fit = pca.fit_transform(X_train_vec)\n",
    "X_train_vec = pd.DataFrame(pca.transform(X_train_vec))\n",
    "X_test_norm_vec = pd.DataFrame(pca.transform(X_test_norm_vec))\n",
    "X_check_vec = pd.DataFrame(pca.transform(X_check_vec))\n",
    "X_test_anom_vec = pd.DataFrame(pca.transform(X_test_anom_vec))\n",
    "X_train_vec.shape,X_test_norm_vec.shape,X_test_anom_vec.shape,X_check_vec.shape,type(X_check_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "pk.dump(pca, open(\"pca.pkl\",\"wb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "(11764, 585)"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = pk.load(open(\"pca.pkl\", 'rb'))\n",
    "df_ = pd.DataFrame(pca.transform(df_))\n",
    "df_.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "(11764, 585)"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_vec = pd.DataFrame(pca.transform(df_test_vec))\n",
    "df_test_vec.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h2kKiIIeBwKb"
   },
   "source": [
    "def create_dataset(df):\n",
    "\n",
    "  sequences = df.astype(np.float32).to_numpy().tolist()\n",
    "  # print(df.shape,type(df))\n",
    "  # torch.tensor(scipy.sparse.csr_matrix.todense(train_in_distribution)).float()\n",
    "  dataset = [torch.tensor(s).unsqueeze(1).float() for s in sequences]\n",
    "  # dataset = [torch.tensor(scipy.sparse.csr_matrix.todense(s)).unsqueeze(1).float() for s in sequences]\n",
    "\n",
    "  n_seq, seq_len, n_features = torch.stack(dataset).shape\n",
    "\n",
    "  return dataset, seq_len, n_features"
   ],
   "execution_count": 118,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "train_vec_dataset, seq_len, n_features = create_dataset(X_train_vec)\n",
    "check_vec_dataset, _, _ = create_dataset(X_check_vec)\n",
    "test_norm_vec_dataset, _, _ = create_dataset(X_test_norm_vec)\n",
    "test_anom_vec_dataset, _, _ = create_dataset(X_test_anom_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [],
   "source": [
    "gen_test_dataset, _, _ = create_dataset(df_test_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [
    {
     "data": {
      "text/plain": "(585, 1)"
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len,n_features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X_f1WaTJhiXy"
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = 2 * embedding_dim\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=n_features, hidden_size=self.hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=self.hidden_dim, hidden_size=embedding_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape((1, self.seq_len, self.n_features))\n",
    "\n",
    "        x, (_, _) = self.lstm1(x)\n",
    "        x, (h_n, _) = self.lstm2(x)\n",
    "        return h_n.reshape((self.n_features, self.embedding_dim))"
   ],
   "execution_count": 257,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AdEft7l3hk6S"
   },
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, input_dim=64, n_features=1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = 2 * input_dim\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=input_dim, hidden_size=input_dim, num_layers=1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=input_dim, hidden_size=self.hidden_dim, num_layers=1, batch_first=True)\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_dim, n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.repeat(self.seq_len, self.n_features)\n",
    "        x = x.reshape((self.n_features, self.seq_len, self.input_dim))\n",
    "\n",
    "        x, (h_n, c_n) = self.lstm1(x)\n",
    "        x, (h_n, c_n) = self.lstm2(x)\n",
    "        x = x.reshape((self.seq_len, self.hidden_dim))\n",
    "        return self.output(x)"
   ],
   "execution_count": 258,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vgUChGd_A-Bv"
   },
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(seq_len, n_features, embedding_dim).to(DEVICE)\n",
    "        self.decoder = Decoder(seq_len, embedding_dim, n_features).to(DEVICE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "execution_count": 259,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zT6Cwq78sOrI"
   },
   "source": [
    "Our Autoencoder passes the input through the Encoder and Decoder. Let's create an instance of it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mo0rvFqRBgnu"
   },
   "source": [
    "model = LSTMAutoencoder(seq_len, n_features, 256)\n",
    "model = model.to(DEVICE)"
   ],
   "execution_count": 260,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1ENnubQdnJN"
   },
   "source": [
    "## Training\n",
    "\n",
    "Let's write a helper function for our training process:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ryEmRvl9DfEj"
   },
   "source": [
    "def train_model(model, train_dataset, check_dataset, epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "\n",
    "    best_model_w = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 10000.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model = model.train()\n",
    "        train_loss_list = list()\n",
    "        for train_d in train_dataset:\n",
    "            optimizer.zero_grad()\n",
    "            train_d = train_d.to(DEVICE)\n",
    "            pred_d = model(train_d)\n",
    "            loss = criterion(pred_d, train_d)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(loss.item())\n",
    "\n",
    "        model = model.eval()\n",
    "        val_loss_list = list()\n",
    "        with torch.no_grad():\n",
    "            for check_d in check_dataset:\n",
    "                check_d = check_d.to(DEVICE)\n",
    "                seq_pred = model(check_d)\n",
    "                loss = criterion(seq_pred, check_d)\n",
    "                val_loss_list.append(loss.item())\n",
    "        train_loss = np.mean(train_loss_list)\n",
    "        val_loss = np.mean(val_loss_list)\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_w = copy.deepcopy(model.state_dict())\n",
    "        print('Epoch {epoch}: train loss {train_loss} val loss {val_loss}'.format(epoch=epoch, train_loss=train_loss,\n",
    "                                                                                  val_loss=val_loss))\n",
    "    model.load_state_dict(best_model_w)\n",
    "    return model.eval()"
   ],
   "execution_count": 261,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "saamYyUsHdw0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0fb227ee-10b8-4e0c-9e27-3280ab3d7527",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model = train_model(model, train_vec_dataset,check_vec_dataset,epochs=150)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 4.379949882604948 val loss 4.081418354494214\n",
      "Epoch 2: train loss 4.304078599584672 val loss 4.040680107958348\n",
      "Epoch 3: train loss 4.306429691817782 val loss 4.086635550147301\n",
      "Epoch 4: train loss 4.303155657549747 val loss 4.051419304875967\n",
      "Epoch 5: train loss 4.303577233690504 val loss 4.094443501442275\n",
      "Epoch 6: train loss 4.3055451886196865 val loss 4.045056180398725\n",
      "Epoch 7: train loss 4.304090342678234 val loss 4.050448175104938\n",
      "Epoch 8: train loss 4.302714952376084 val loss 4.051096580123512\n",
      "Epoch 9: train loss 4.303396956434807 val loss 4.052989974815828\n",
      "Epoch 10: train loss 4.304124182833962 val loss 4.083509454201142\n",
      "Epoch 11: train loss 4.30641805141344 val loss 4.05071648513942\n",
      "Epoch 12: train loss 4.303329603992068 val loss 4.050594602347151\n",
      "Epoch 13: train loss 4.304686496909375 val loss 4.060787711615947\n",
      "Epoch 14: train loss 4.304070798225992 val loss 4.051714015810677\n",
      "Epoch 15: train loss 4.306213473824704 val loss 4.073600132204782\n",
      "Epoch 16: train loss 4.3030701077312505 val loss 4.0792928757536036\n",
      "Epoch 17: train loss 4.304186272310324 val loss 4.0578735035456965\n",
      "Epoch 18: train loss 4.306537317372013 val loss 4.049571061402224\n",
      "Epoch 19: train loss 4.3066542712690055 val loss 4.05062190626688\n",
      "Epoch 20: train loss 4.303456140076809 val loss 4.086436295777224\n",
      "Epoch 21: train loss 4.30472909589714 val loss 4.050472496235334\n",
      "Epoch 22: train loss 4.304256656905523 val loss 4.1909020314786485\n",
      "Epoch 23: train loss 4.303826283395492 val loss 4.058166741837764\n",
      "Epoch 24: train loss 4.303735558535384 val loss 4.047606669358749\n",
      "Epoch 25: train loss 4.304425037983591 val loss 4.0510499231418375\n",
      "Epoch 26: train loss 4.304166816041874 val loss 4.057700788889526\n",
      "Epoch 27: train loss 4.304244623904871 val loss 4.052583091714409\n",
      "Epoch 28: train loss 4.305044813555603 val loss 4.105278260617748\n",
      "Epoch 29: train loss 4.3062735963190075 val loss 4.051026491393108\n",
      "Epoch 30: train loss 4.305949935528577 val loss 4.073429677532691\n",
      "Epoch 31: train loss 4.306160719454548 val loss 4.050728619403079\n",
      "Epoch 32: train loss 4.304788923534088 val loss 4.050882603954125\n",
      "Epoch 33: train loss 4.301835314770472 val loss 4.113145334849684\n",
      "Epoch 34: train loss 4.3052273226613185 val loss 4.052103536243458\n",
      "Epoch 35: train loss 4.303445389453827 val loss 4.079025473122212\n",
      "Epoch 36: train loss 4.302617568028025 val loss 4.051615750339106\n",
      "Epoch 37: train loss 4.306560965766658 val loss 4.104367495313729\n",
      "Epoch 38: train loss 4.305810726132155 val loss 4.058027713121018\n",
      "Epoch 39: train loss 4.3051648692253535 val loss 4.192114718479083\n",
      "Epoch 40: train loss 4.305741808455903 val loss 4.045252092281571\n",
      "Epoch 41: train loss 4.305275022868429 val loss 4.050994453439917\n",
      "Epoch 42: train loss 4.305193853976933 val loss 4.091560165777391\n",
      "Epoch 43: train loss 4.302429416742606 val loss 4.052617220152873\n",
      "Epoch 44: train loss 4.304283332675048 val loss 4.0515948754409\n",
      "Epoch 45: train loss 4.304088660912028 val loss 4.083515034267437\n",
      "Epoch 46: train loss 4.304115050890088 val loss 4.047647025417137\n",
      "Epoch 47: train loss 4.305315304873129 val loss 4.050955942385774\n",
      "Epoch 48: train loss 4.306339672140198 val loss 4.0734732082842315\n",
      "Epoch 49: train loss 4.303480201617121 val loss 4.093911098143176\n",
      "Epoch 50: train loss 4.3062085373743555 val loss 4.061599625508559\n",
      "Epoch 51: train loss 4.304315586990345 val loss 4.0507794237477786\n",
      "Epoch 52: train loss 4.3039364746260445 val loss 4.078950083657597\n",
      "Epoch 53: train loss 4.305665532037983 val loss 4.058220567693506\n",
      "Epoch 54: train loss 4.305170537221265 val loss 4.052595538430609\n",
      "Epoch 55: train loss 4.305546757767363 val loss 4.039626800003285\n",
      "Epoch 56: train loss 4.305152727318869 val loss 4.083619444070725\n",
      "Epoch 57: train loss 4.306002252519793 val loss 4.051241895882175\n",
      "Epoch 58: train loss 4.303606658958111 val loss 4.087480613476653\n",
      "Epoch 59: train loss 4.304933510443717 val loss 4.086941033997501\n",
      "Epoch 60: train loss 4.305282949250082 val loss 4.092755717081727\n",
      "Epoch 61: train loss 4.306189793166994 val loss 4.129970895866573\n",
      "Epoch 62: train loss 4.3036485814000605 val loss 4.047664031797337\n",
      "Epoch 63: train loss 4.303943777331864 val loss 4.093911503138167\n",
      "Epoch 64: train loss 4.3074147546711545 val loss 4.050700074683902\n",
      "Epoch 65: train loss 4.30268140371038 val loss 4.051888116898892\n",
      "Epoch 66: train loss 4.304734820705978 val loss 4.071656003548736\n",
      "Epoch 67: train loss 4.304346371479854 val loss 4.092178827408507\n",
      "Epoch 68: train loss 4.304324587982441 val loss 4.087970939914832\n",
      "Epoch 69: train loss 4.302707528417551 val loss 4.189732506277612\n",
      "Epoch 70: train loss 4.306770504931215 val loss 4.052723403118239\n",
      "Epoch 71: train loss 4.304703591268227 val loss 4.036298968940517\n",
      "Epoch 72: train loss 4.308013244621538 val loss 4.050503916102362\n",
      "Epoch 73: train loss 4.306948084177343 val loss 4.050705438736632\n",
      "Epoch 74: train loss 4.305027653461836 val loss 4.087857111968838\n",
      "Epoch 75: train loss 4.30188915028427 val loss 4.047473632035143\n",
      "Epoch 76: train loss 4.3052861797872914 val loss 4.07389290145274\n",
      "Epoch 77: train loss 4.306398444104344 val loss 4.16947350122103\n",
      "Epoch 78: train loss 4.3036408888320885 val loss 4.078046363025934\n",
      "Epoch 79: train loss 4.303740128522216 val loss 4.047463421680345\n",
      "Epoch 80: train loss 4.3054622953379 val loss 4.087949113047038\n",
      "Epoch 81: train loss 4.306541262790812 val loss 4.060662972914917\n",
      "Epoch 82: train loss 4.304948297029299 val loss 4.060268984386456\n",
      "Epoch 83: train loss 4.305215070122166 val loss 4.0793733869558455\n",
      "Epoch 84: train loss 4.304739416854385 val loss 4.057270683234023\n",
      "Epoch 85: train loss 4.302907861988651 val loss 4.056528255571749\n",
      "Epoch 86: train loss 4.303508212634298 val loss 4.08617846885424\n",
      "Epoch 87: train loss 4.305051971149583 val loss 4.0603721643492685\n",
      "Epoch 88: train loss 4.303606888512262 val loss 4.073587109046523\n",
      "Epoch 89: train loss 4.302967154996974 val loss 4.050467971152987\n",
      "Epoch 90: train loss 4.3044359649165935 val loss 4.050658586441679\n",
      "Epoch 91: train loss 4.302956651944806 val loss 4.051202873165688\n",
      "Epoch 92: train loss 4.303484306710748 val loss 4.072851452812841\n",
      "Epoch 93: train loss 4.305186384026984 val loss 4.050689733552007\n",
      "Epoch 94: train loss 4.304364950601632 val loss 4.050587974359359\n",
      "Epoch 95: train loss 4.30331495995685 val loss 4.091554911557939\n",
      "Epoch 96: train loss 4.304845598571719 val loss 4.082772550105561\n",
      "Epoch 97: train loss 4.304599541704877 val loss 4.091839721424465\n",
      "Epoch 98: train loss 4.306248571175069 val loss 4.060774512383497\n",
      "Epoch 99: train loss 4.304189632187599 val loss 4.058403720651145\n",
      "Epoch 100: train loss 4.30452631079362 val loss 4.052495419674679\n",
      "Epoch 101: train loss 4.305273845568767 val loss 4.050479757651854\n",
      "Epoch 102: train loss 4.302693261437114 val loss 4.062372023043764\n",
      "Epoch 103: train loss 4.3053908389815385 val loss 4.051163951758832\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tLC_ClIpnv9H",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "torch.save(model, 'model.pt')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e4Hxo-Xftiej",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# model = torch.load('model.pt')\n",
    "# model = model.to(DEVICE)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AAhYZy9bRNLM",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def predict(model, dataset):\n",
    "    predictions = list()\n",
    "    loss_list = list()\n",
    "    criterion = nn.L1Loss(reduction='sum').to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for seq_true in dataset:\n",
    "            seq_true = seq_true.to(DEVICE)\n",
    "            seq_pred = model(seq_true)\n",
    "            loss = criterion(seq_pred, seq_true)\n",
    "            predictions.append(seq_pred.cpu().numpy().flatten())\n",
    "            loss_list.append(loss.item())\n",
    "    return predictions, loss_list"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pvn141SDS33P",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "_, loss_list = predict(model, train_vec_dataset)\n",
    "sns.distplot(loss_list, bins=100, kde=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MjSCtDZ8_xGB",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "THRESHOLD = 4.5"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-z630B5v7Fid",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "predictions, pred_norm_loss_list = predict(model, test_norm_vec_dataset)\n",
    "sns.distplot(pred_norm_loss_list, bins=100, kde=True);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BR-hcvUP7OBt",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "correct = sum(l <= THRESHOLD for l in pred_norm_loss_list)\n",
    "print('Correct normal predictions:', correct,'/',len(test_norm_vec_dataset))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WJcg5DXWyiep",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "anomaly_dataset = test_anom_vec_dataset[:len(test_norm_vec_dataset)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tLCuS8oL7hG2",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "predictions, pred_anom_loss_list = predict(model, anomaly_dataset)\n",
    "sns.distplot(pred_anom_loss_list, bins=100, kde=True);"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correct = sum(l <= THRESHOLD for l in anomaly_dataset)\n",
    "print('Correct anomaly predictions:', correct,'/',len(anomaly_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions, pred_loss_list = predict(model, gen_test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result=pd.DataFrame({'y_true':[0 if l<=THRESHOLD else 1 for l in pred_loss_list]})\n",
    "result.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ]
}